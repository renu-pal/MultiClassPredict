{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import itertools\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from lightgbm import LGBMClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    f_classif,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, label_binarize, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from tabpfn import TabPFNClassifier\n",
    "# from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import combinations\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OvO and OvR prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_classes(X, y):\n",
    "    return {\n",
    "        (c1, c2): (X[(y == c1) | (y == c2)], y[(y == c1) | (y == c2)])\n",
    "        for c1, c2 in itertools.combinations(np.unique(y), 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def ovo_and_ova_multiclass_auc(X, y, base_clf, p_grid, random_state):\n",
    "    results = {}\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    class_names = le.classes_\n",
    "\n",
    "    # Stratified K-Folds\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    ####################\n",
    "    # One-vs-Rest Classification\n",
    "    ####################\n",
    "    print(\"Performing One vs Rest classification\")\n",
    "\n",
    "    #checking grid search enabled or not \n",
    "    if p_grid is not None:\n",
    "        ovr_clf = GridSearchCV(\n",
    "            estimator=OneVsRestClassifier(base_clf),\n",
    "            param_grid=p_grid,\n",
    "            cv=inner_cv,\n",
    "            scoring=\"roc_auc_ovr\",\n",
    "        )\n",
    "    else:\n",
    "        ovr_clf = OneVsRestClassifier(base_clf)\n",
    "   \n",
    "    \n",
    "    y_score = cross_val_predict(ovr_clf, X, y_encoded, cv=outer_cv, method=\"predict_proba\") \n",
    "    y_pred = np.argmax(y_score, axis=1) \n",
    "    \n",
    "    # Per-class metrics for OvR \n",
    "    per_class_precision = [] \n",
    "    per_class_recall = [] \n",
    "    per_class_f1 = [] \n",
    "    per_class_mcc = []\n",
    "\n",
    "    for idx, cls in enumerate(class_names): \n",
    "        y_bin = (y_encoded == idx).astype(int) \n",
    "        cls_score = y_score[:, idx] \n",
    "        \n",
    "        # Ensure minority class is positive \n",
    "        if np.sum(y_bin) > np.sum(1 - y_bin): \n",
    "            y_bin = 1 - y_bin \n",
    "            cls_score = 1 - cls_score \n",
    "            \n",
    "        y_pred_bin = (y_pred == idx).astype(int) \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_bin, y_pred_bin, average=\"binary\") \n",
    "        mcc = matthews_corrcoef(y_bin, y_pred_bin) \n",
    "        prec_curve, rec_curve, _ = precision_recall_curve(y_bin, cls_score) \n",
    "        pr_auc_val = auc(rec_curve, prec_curve) \n",
    "        roc_auc_val = roc_auc_score(y_bin, cls_score) \n",
    "        \n",
    "        results[f\"{cls} vs Rest - Precision\"] = precision \n",
    "        results[f\"{cls} vs Rest - Recall\"] = recall \n",
    "        results[f\"{cls} vs Rest - F1\"] = f1 \n",
    "        results[f\"{cls} vs Rest - MCC\"] = mcc \n",
    "        results[f\"{cls} vs Rest - PR AUC\"] = pr_auc_val \n",
    "        results[f\"{cls} vs Rest - AUC\"] = roc_auc_val \n",
    "        \n",
    "        per_class_precision.append(precision) \n",
    "        per_class_recall.append(recall) \n",
    "        per_class_f1.append(f1) \n",
    "        per_class_mcc.append(mcc) \n",
    "    \n",
    "    # Macro metrics OvR \n",
    "\n",
    "    macro_ovr_auc = np.mean([results[f\"{cls} vs Rest - AUC\"] for cls in class_names]) \n",
    "    macro_ovr_precision = np.mean(per_class_precision) \n",
    "    macro_ovr_recall = np.mean(per_class_recall) \n",
    "    macro_ovr_f1 = np.mean(per_class_f1) \n",
    "    macro_ovr_mcc = np.mean(per_class_mcc)\n",
    "    macro_ovr_pr_auc = np.mean([results[f\"{cls} vs Rest - PR AUC\"] for cls in class_names])\n",
    "\n",
    "    \n",
    "    results[\"OvR Macro AUC\"] = macro_ovr_auc\n",
    "    results[\"OvR Macro Precision\"] = macro_ovr_precision\n",
    "    results[\"OvR Macro Recall\"] = macro_ovr_recall\n",
    "    results[\"OvR Macro F1\"] = macro_ovr_f1 \n",
    "    results[\"OvR Macro MCC\"] =  macro_ovr_mcc\n",
    "    results[\"OvR Macro PR AUC\"] = macro_ovr_pr_auc \n",
    "\n",
    "    print(f\"Macro AUC (OvR): {macro_ovr_auc:.4f}\")\n",
    "    print(f\"Macro Precision (OvR): {macro_ovr_precision:.4f}\")\n",
    "    print(f\"Macro Recall (OvR): {macro_ovr_recall:.4f}\")\n",
    "    print(f\"Macro F1 (OvR): {macro_ovr_f1:.4f}\")\n",
    "    print(f\"Macro MCC (OvR): {macro_ovr_mcc:.4f}\")\n",
    "    print(f\"Macro PR AUC (OvR): {macro_ovr_pr_auc:.4f}\")\n",
    "\n",
    "    ####################\n",
    "    # One-vs-One Classification\n",
    "    ####################\n",
    "    print(\"Performing One vs One classification\")\n",
    "\n",
    "    ovo_auc = {} \n",
    "    ovo_precision = {} \n",
    "    ovo_recall = {} \n",
    "    ovo_f1 = {} \n",
    "    ovo_mcc = {} \n",
    "    \n",
    "    for c1, c2 in combinations(range(len(class_names)), 2): \n",
    "        mask = np.isin(y_encoded, [c1, c2]) \n",
    "        X_pair, y_pair = X[mask], y_encoded[mask] \n",
    "\n",
    "        # checking grid search enabled or not\n",
    "        if p_grid is not None:\n",
    "            ovo_clf = GridSearchCV(\n",
    "                estimator=base_clf,\n",
    "                param_grid={k.replace(\"estimator__\", \"\"): v for k, v in p_grid.items()},\n",
    "                cv=inner_cv,\n",
    "                scoring=\"roc_auc\"\n",
    "            )\n",
    "        else:\n",
    "            ovo_clf = base_clf\n",
    "            \n",
    "        y_score_pair = cross_val_predict(ovo_clf, X_pair, y_pair, cv=outer_cv, method=\"predict_proba\") \n",
    "        \n",
    "        # Identify minority \n",
    "        \n",
    "        vals, counts = np.unique(y_pair, return_counts=True) \n",
    "        minority = vals[np.argmin(counts)] \n",
    "        minority_idx = np.where([c1, c2] == minority)[0][0] \n",
    "        \n",
    "        y_bin = (y_pair == minority).astype(int) \n",
    "        y_score_cls = y_score_pair[:, minority_idx] \n",
    "        \n",
    "        # Ensure minority positive \n",
    "        \n",
    "        if np.sum(y_bin) > np.sum(1 - y_bin): \n",
    "            y_bin = 1 - y_bin \n",
    "            y_score_cls = 1 - y_score_cls \n",
    "            \n",
    "        y_pred_bin = (np.argmax(y_score_pair, axis=1) == minority_idx).astype(int)\n",
    "                                                                             \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_bin, y_pred_bin, average=\"binary\") \n",
    "        mcc = matthews_corrcoef(y_bin, y_pred_bin) \n",
    "        prec_curve, rec_curve, _ = precision_recall_curve(y_bin, y_score_cls) \n",
    "        pr_auc_val = auc(rec_curve, prec_curve) \n",
    "        roc_auc_val = roc_auc_score(y_bin, y_score_cls)\n",
    "        \n",
    "        pair_name = f\"{le.inverse_transform([c1])[0]} vs {le.inverse_transform([c2])[0]}\" \n",
    "        \n",
    "        results[f\"{pair_name} - Precision\"] = precision \n",
    "        results[f\"{pair_name} - Recall\"] = recall \n",
    "        results[f\"{pair_name} - F1\"] = f1 \n",
    "        results[f\"{pair_name} - MCC\"] = mcc \n",
    "        results[f\"{pair_name} - PR AUC\"] = pr_auc_val \n",
    "        results[f\"{pair_name} - AUC\"] = roc_auc_val \n",
    "        \n",
    "        ovo_auc[(c1, c2)] = roc_auc_val \n",
    "        ovo_precision[(c1, c2)] = precision \n",
    "        ovo_recall[(c1, c2)] = recall \n",
    "        ovo_f1[(c1, c2)] = f1 \n",
    "        ovo_mcc[(c1, c2)] = mcc \n",
    "        \n",
    "        \n",
    "    # Macro metrics OvO \n",
    "    macro_ovo_auc = np.mean(list(ovo_auc.values()))\n",
    "    macro_ovo_precision = np.mean(list(ovo_precision.values()))\n",
    "    macro_ovo_recall = np.mean(list(ovo_recall.values())) \n",
    "    macro_ovo_f1 = np.mean(list(ovo_f1.values())) \n",
    "    macro_ovo_mcc = np.mean(list(ovo_mcc.values())) \n",
    "    macro_ovo_pr_auc = np.mean([results[k] for k in results if \"vs\" in k and \"PR AUC\" in k]) \n",
    "\n",
    "    results[\"OvO Macro AUC\"] =  macro_ovo_auc\n",
    "    results[\"OvO Macro Precision\"] = macro_ovo_precision\n",
    "    results[\"OvO Macro Recall\"] = macro_ovo_recall\n",
    "    results[\"OvO Macro F1\"] = macro_ovo_f1\n",
    "    results[\"OvO Macro MCC\"] = macro_ovo_mcc\n",
    "    results[\"OvO Macro PR AUC\"] =  macro_ovo_pr_auc\n",
    "\n",
    "\n",
    "    print(f\"Macro AUC (OvO): {macro_ovo_auc:.4f}\")\n",
    "    print(f\"Macro Precision (OvO): {macro_ovo_precision:.4f}\")\n",
    "    print(f\"Macro Recall (OvO): {macro_ovo_recall:.4f}\")\n",
    "    print(f\"Macro F1 (OvO): {macro_ovo_f1:.4f}\")\n",
    "    print(f\"Macro MCC (OvO): {macro_ovo_mcc:.4f}\")\n",
    "    print(f\"Macro PR AUC (OvO): {macro_ovo_pr_auc:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def repeat_clf(n_seeds, ks, X, y, label, model, sampling_strategy, use_grid=False):\n",
    "\n",
    "    print(ks)\n",
    "    print(n_seeds)\n",
    "\n",
    "    # Define sampling strategies\n",
    "    sampling_strategies = {\n",
    "        \"No Sampling\": None,\n",
    "        \"Random OverSampling\": RandomOverSampler(random_state=42),\n",
    "        \"SMOTE\": SMOTE(random_state=42),\n",
    "        \"Random UnderSampling\": RandomUnderSampler(random_state=42),\n",
    "        \"NearMiss (v1)\": NearMiss(version=1),\n",
    "        \"NearMiss (v2)\": NearMiss(version=2),\n",
    "        \"NearMiss (v3)\": NearMiss(version=3),\n",
    "    }\n",
    "\n",
    "    # If the selected strategy is not in the dictionary, use \"No Sampling\"\n",
    "    sampler = sampling_strategies.get(sampling_strategy, None)\n",
    "\n",
    "    seed_results = {}\n",
    "\n",
    "    for seed in range(n_seeds):\n",
    "\n",
    "        ks_results = {}\n",
    "        for k in ks:\n",
    "\n",
    "            print(f\"CV for seed {seed} and {k} features\")\n",
    "\n",
    "            # Create a Random Forest Classifier\n",
    "            rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "            # Create a SelectFromModel using the Random Forest Classifier\n",
    "            selector = SelectFromModel(rf, max_features=k)\n",
    "\n",
    "            if model == \"rf\":\n",
    "                ml_model = rf\n",
    "                ml_model_grid = {\n",
    "                    \"estimator__classification__n_estimators\":[100, 300, 500],  # Number of trees in the forest\n",
    "                    \"estimator__classification__max_depth\": [None, 10, 20, 30],  # tree depth\n",
    "                    \"estimator__classification__max_features\": [\"sqrt\", \"log2\"],  # Feature selection strategy\n",
    "                    \"estimator__classification__criterion\": [\"entropy\"],  # Split criterion\n",
    "                    \"estimator__classification__min_samples_leaf\": [1, 2, 4],  # Minimum samples per leaf\n",
    "                }\n",
    "            elif model == \"xgb\":\n",
    "                ml_model = XGBClassifier(\n",
    "                    use_label_encoder=False, eval_metric=\"logloss\", random_state=seed\n",
    "                )\n",
    "                ml_model_grid = {\n",
    "                    \"estimator__classification__n_estimators\": [100, 300, 500], \n",
    "                    \"estimator__classification__gamma\": [0, 0.1, 0.3], # min loss reduction\n",
    "                    \"estimator__classification__max_depth\": [3, 5, 7], \n",
    "                    \"estimator__classification__learning_rate\": [0.01, 0.05, 0.1], # step size\n",
    "                }\n",
    "            elif model == \"etc\":\n",
    "                ml_model = ExtraTreesClassifier(random_state=seed)\n",
    "                ml_model_grid = {\n",
    "                    \"estimator__classification__n_estimators\": [100, 300, 500],\n",
    "                    \"estimator__classification__max_depth\": [None, 10, 20],       # tree depth\n",
    "                    \"estimator__classification__max_features\": [\"sqrt\", \"log2\"],  # features per split\n",
    "                    \"estimator__classification__min_samples_leaf\": [1, 2, 4],     # min leaf samples\n",
    "                    \n",
    "                }\n",
    "            elif model == \"lgbm\":\n",
    "                ml_model = LGBMClassifier(random_state=seed, verbose=-1)\n",
    "                ml_model_grid = {\n",
    "                    \"estimator__classification__n_estimators\": [100, 300, 500],  \n",
    "                    \"estimator__classification__learning_rate\": [0.01, 0.05, 0.1],\n",
    "                    \"estimator__classification__num_leaves\": [31, 63, 127],      # leaves per tree\n",
    "                            \n",
    "                }\n",
    "\n",
    "            # If there is a sampler, include it in the pipeline\n",
    "            steps = []\n",
    "            if sampler:\n",
    "                steps.append((\"sampling\", sampler))\n",
    "            steps.append((\"feature_selection\", selector))\n",
    "            steps.append((\"classification\", ml_model))\n",
    "\n",
    "            # Create a pipeline with feature selection, sampling, and classification\n",
    "            pipeline = Pipeline(steps=steps)\n",
    "\n",
    "            ###########################\n",
    "\n",
    "            # Run the classification with the sampling strategy\n",
    "            if use_grid:\n",
    "                results = ovo_and_ova_multiclass_auc(\n",
    "                    X, y, pipeline, ml_model_grid, random_state=seed\n",
    "                )\n",
    "            else:\n",
    "                results = ovo_and_ova_multiclass_auc(\n",
    "                    X, y, pipeline, None, random_state=seed\n",
    "                )\n",
    "                       \n",
    "\n",
    "            print(results)\n",
    "\n",
    "            ks_results[k] = {\n",
    "                \"results\": results,\n",
    "                \"Label\": label,\n",
    "                \"Model\": model,\n",
    "                \"Sampling_Strategy\": sampling_strategy,\n",
    "            }\n",
    "\n",
    "\n",
    "        seed_results[seed] = copy.copy(ks_results)\n",
    "\n",
    "    return seed_results\n",
    "\n",
    "\n",
    "def store_results(seed_results, output):\n",
    "\n",
    "    # Flatten the nested dictionary into a DataFrame\n",
    "    '''df = pd.DataFrame(\n",
    "        {\n",
    "            (outer_key, inner_key): values\n",
    "            for outer_key, inner_dict in seed_results.items()\n",
    "            for inner_key, values in inner_dict.items()\n",
    "        }\n",
    "    ).T\n",
    "\n",
    "    # '''\n",
    "    \n",
    "    final_results = []\n",
    "    metrics = [\"AUC\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"PR\"]\n",
    "    \n",
    "    for seed, ks_results in seed_results.items():\n",
    "        for k, result_info in ks_results.items():\n",
    "            result = result_info[\"results\"]\n",
    "            model = result_info[\"Model\"]\n",
    "            sampling_strategy = result_info[\"Sampling_Strategy\"]\n",
    "            label=result_info[\"Label\"]\n",
    "                        \n",
    "            # Determine Class and Type\n",
    "           \n",
    "            groups = set()\n",
    "            for key in result.keys():\n",
    "                if \"Macro\" in key:\n",
    "                    groups.add((\"Macro\", \"OvR\" if \"OvR\" in key else \"OvO\"))\n",
    "                elif \"vs Rest\" in key:\n",
    "                    groups.add((key.split(\" vs Rest\")[0], \"OvR\"))\n",
    "                else:\n",
    "                    groups.add((key.split(\" - \")[0], \"OvO\"))\n",
    "\n",
    "            # assign metric values according to class and type\n",
    "            for class_name, type_name in groups:\n",
    "\n",
    "                metric_values = {}\n",
    "            \n",
    "                for metric in metrics:\n",
    "                    metric_key = None\n",
    "            \n",
    "                    for key in result.keys():\n",
    "                        if class_name == \"Macro\":\n",
    "                            if metric in key and \"Macro\" in key and type_name in key:\n",
    "                                metric_key = key\n",
    "                                break\n",
    "                        elif type_name == \"OvR\":\n",
    "                            if metric in key and f\"{class_name} vs Rest\" in key:\n",
    "                                metric_key = key\n",
    "                                break\n",
    "                        else:  # OvO\n",
    "                            if metric in key and \"vs\" in key and class_name in key:\n",
    "                                metric_key = key\n",
    "                                break\n",
    "            \n",
    "                    metric_values[metric] = result[metric_key] if metric_key else np.nan\n",
    "\n",
    "                final_results.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Features (k)\": k,\n",
    "                    \"Label\": label,\n",
    "                    \"Model\": model,\n",
    "                    \"Sampling_Strategy\": sampling_strategy,\n",
    "                    \"Class\": class_name,\n",
    "                    \"Type\": type_name,\n",
    "                    **metric_values\n",
    "                })\n",
    "                \n",
    "    df = pd.DataFrame(final_results)\n",
    "\n",
    "    '''#Set multi-level index names for clarity\n",
    "    df.set_index([\"Seed\", \"Features (k)\", \"Label\", \"Model\", \"Sampling_Strategy\"], inplace=True)\n",
    "\n",
    "    df.index.names = [\"Seed\", \"Features (k)\",\"Label\",\"Model\",\"Sampling_Strategy\"]\n",
    "    # Display the DataFrame\n",
    "    df = df.reset_index()'''\n",
    "\n",
    "    df.to_csv(output, mode='a', header=not os.path.exists(output), index=False)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "\n",
    "def run_classification(X, y, ks, n_seeds,output, label,model, sampling_strategy,use_grid=False):\n",
    "\n",
    "    # Ensure ks does not exceed the number of columns in X\n",
    "    max_features = len(X.columns)\n",
    "    ks = [k for k in ks if k <= max_features]\n",
    "    if max_features not in ks:\n",
    "        ks.append(max_features)\n",
    "\n",
    "    seed_results = repeat_clf(n_seeds, ks, X, y, label,model, sampling_strategy, use_grid=use_grid)\n",
    "    store_results(seed_results, output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import omics and metadata data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 omics datasets:\n",
      " - SCFA: 134 samples × 27 features\n",
      " - Metabolites: 134 samples × 439 features\n",
      " - GCMS: 134 samples × 99 features\n",
      " - Lipids: 134 samples × 622 features\n",
      " - 16S: 134 samples × 6241 features\n",
      " - RNA: 134 samples × 60839 features\n",
      "Metadata loaded with shape: (134, 31)\n",
      "\n",
      "Dataset loading complete. (Virulence-Genes excluded)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define folder\n",
    "output_dir = \"HP_analysis_10_2025/input/HP_multiomics/\"\n",
    "\n",
    "# Step 2: Initialize list to store omics datasets\n",
    "raw_data = []\n",
    "\n",
    "# Step 3: Loop through all TSV files (excluding metadata and Virulence-Genes)\n",
    "for file in os.listdir(output_dir):\n",
    "    if (\n",
    "        file.endswith(\"_clean.tsv\") \n",
    "        and file != \"metadata_clean.tsv\"\n",
    "        and not file.startswith(\"Virulence-Genes\")\n",
    "    ):\n",
    "        name = file.replace(\"_clean.tsv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(output_dir, file), sep=\"\\t\", index_col=0)\n",
    "        raw_data.append({\"name\": name, \"df\": df})\n",
    "\n",
    "# Step 4: Load metadata separately\n",
    "metadata_path = os.path.join(output_dir, \"metadata_clean.tsv\")\n",
    "y_aligned = pd.read_csv(metadata_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "# Step 5: Print summary\n",
    "print(f\"Loaded {len(raw_data)} omics datasets:\")\n",
    "for omic in raw_data:\n",
    "    print(f\" - {omic['name']}: {omic['df'].shape[0]} samples × {omic['df'].shape[1]} features\")\n",
    "\n",
    "print(f\"Metadata loaded with shape: {y_aligned.shape}\")\n",
    "print(\"\\nDataset loading complete. (Virulence-Genes excluded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply TSS and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Total Sum Scaling function ---\n",
    "def total_sum_scale(df):\n",
    "    df = df.fillna(0)\n",
    "    return df.div(df.sum(axis=1), axis=0)\n",
    "\n",
    "# --- Prepare TSS datasets ---\n",
    "tss_datasets = []\n",
    "dataset_names = []\n",
    "\n",
    "for omics_data in raw_data:\n",
    "    df_tss = total_sum_scale(omics_data[\"df\"])\n",
    "    tss_datasets.append(df_tss)\n",
    "    dataset_names.append(omics_data[\"name\"])\n",
    "\n",
    "# --- Combined dataset ---\n",
    "df_combined = pd.concat(tss_datasets, axis=1)\n",
    "tss_datasets.append(df_combined)\n",
    "dataset_names.append(\"Combined\")\n",
    "\n",
    "# --- Encode target ---\n",
    "y_target = y_aligned[\"Sample_Condition\"]\n",
    "\n",
    "#print(df_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test combined dataset - on all 4 models (no grid search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 2\n",
    "ks = [10]\n",
    "result_path=\"HP_analysis_10_2025/benchmark/results/diff_models_on_combined(no grid).csv\"\n",
    "# Check if the file exists and remove it once\n",
    "if os.path.exists(result_path):\n",
    "    print(f\"File {result_path} exists. Deleting it to start fresh.\")\n",
    "    os.remove(result_path)\n",
    "    \n",
    "for model in ['rf', 'xgb', 'etc', 'lgbm']:\n",
    "    print(f\"model: {model} \")\n",
    "    print(\"Combined dataset scores:\")\n",
    "    run_classification(df_combined, y_target, ks, n_seeds,result_path,\"combined\", model, None,use_grid=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model Performance by Metric (Macro Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the results\n",
    "df = pd.read_csv(\"HP_analysis_10_2025/benchmark/results/diff_models_on_combined(no grid).csv\")\n",
    "\n",
    "# Use only Macro scores to compare models\n",
    "metrics = ['AUC', 'Precision', 'Recall', 'F1', 'MCC', 'PR']\n",
    "df_macro = df[df['Class'] == 'Macro']\n",
    "\n",
    "# Melt the dataframe for easier plotting\n",
    "df_melt = df_macro.melt(id_vars=['Model'], value_vars=metrics, var_name='Metric', value_name='Score')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_melt, x='Metric', y='Score', hue='Model', ci=None, edgecolor='black', linewidth=0.5)\n",
    "plt.title(\"Model Performance by Metric (Macro Scores)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)  \n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 out of n Dataset combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total combinations :  15\n",
      "[['SCFA', 'Metabolites'], ['SCFA', 'GCMS'], ['SCFA', 'Lipids'], ['SCFA', '16S'], ['SCFA', 'RNA'], ['Metabolites', 'GCMS'], ['Metabolites', 'Lipids'], ['Metabolites', '16S'], ['Metabolites', 'RNA'], ['GCMS', 'Lipids'], ['GCMS', '16S'], ['GCMS', 'RNA'], ['Lipids', '16S'], ['Lipids', 'RNA'], ['16S', 'RNA']]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Exclude the already combined dataset at the end\n",
    "num_datasets = len(tss_datasets) - 1\n",
    "\n",
    "all_combinations = []  # Stores the actual dataframes for each combination\n",
    "all_combination_names = []  # Stores the names of datasets in each combination\n",
    "\n",
    "# Generate all 2-dataset combinations\n",
    "for combo_indices in itertools.combinations(range(num_datasets), 2):\n",
    "    combination_df = pd.concat([tss_datasets[i] for i in combo_indices], axis=1)\n",
    "    all_combinations.append(combination_df) # Store the dataframe separately\n",
    "\n",
    "     # Store the names of the datasets in this combination\n",
    "    all_combination_names.append([dataset_names[i] for i in combo_indices]) \n",
    "print(\"total combinations : \", len(all_combination_names))\n",
    "print(all_combination_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the best model from above (RF) across different dataset combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 2\n",
    "ks = [10]\n",
    "result_path = \"HP_analysis_10_2025/benchmark/results/combinations_of_2_datasets(no_grid).csv\"\n",
    "\n",
    "# Check if the file exists and remove it once\n",
    "if os.path.exists(result_path):\n",
    "    print(f\"File {result_path} exists. Deleting it to start fresh.\")\n",
    "    os.remove(result_path)\n",
    "\n",
    "# Loop through all dataset combinations\n",
    "for combo_df, combo_names in zip(all_combinations, all_combination_names):\n",
    "    combo_label = \"+\".join(combo_names)  # e.g., \"metabolomics+transcriptomics\"\n",
    "    print(f\"\\nRunning rf model on combination: {combo_label}\")\n",
    "    run_classification(combo_df, y_target, ks, n_seeds, result_path, combo_label, \"rf\", None, use_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Macro Scores Across 2-Dataset Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the results\n",
    "df = pd.read_csv(\"HP_analysis_10_2025/benchmark/results/combinations_of_2_datasets(no_grid).csv\")\n",
    "\n",
    "# Keep only Macro class scores\n",
    "df_macro = df[df['Class'] == 'Macro']\n",
    "\n",
    "# List of metrics to plot (adjust based on your CSV)\n",
    "metrics = ['AUC', 'Precision', 'Recall', 'F1', 'MCC', 'PR']\n",
    "\n",
    "#  Ranked Barplots per metric ---\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    df_sorted = df_macro.sort_values(metric, ascending=False)\n",
    "    sns.barplot(data=df_sorted, x='Label', y=metric, palette='viridis', ci=None)\n",
    "    plt.title(f\"{metric} Ranked Across Dataset Combinations (RF Model)\")\n",
    "    plt.ylabel(f\" Macro {metric} \")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel(\"Dataset Combination\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all Metric scores for top 5 dataset combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "df = pd.read_csv(\"HP_analysis_10_2025/benchmark/results/combinations_of_2_datasets(no_grid).csv\")\n",
    "\n",
    "# Keep only Macro scores\n",
    "df_macro = df[df['Class'] == 'Macro']\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['AUC', 'Precision', 'Recall', 'F1', 'MCC', 'PR']\n",
    "\n",
    "# Compute overall mean per label\n",
    "label_means = df_macro.groupby('Label')[metrics].mean().reset_index()\n",
    "\n",
    "# Select top 5 dataset combinations\n",
    "label_means['Overall'] = label_means[metrics].mean(axis=1)\n",
    "top5_df = label_means.sort_values('Overall', ascending=False).head(5)\n",
    "\n",
    "\n",
    "# Melt the dataframe for seaborn plotting\n",
    "df_long = top5_df.melt(id_vars='Label', value_vars=metrics, var_name='Metric', value_name='Score')\n",
    "\n",
    "\n",
    "# Plot grouped barplot for top 5 combinations\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=df_long, x='Label', y='Score', hue='Metric', ci=None)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Top 5 Dataset Combinations Across All Metrics (RF Model)\")\n",
    "plt.xlabel(\"Dataset Combination\")\n",
    "plt.ylabel(\"Macro Scores\")\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Random Forest using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 2\n",
    "ks = [10]\n",
    "result_path=\"HP_analysis_10_2025/benchmark/results/Hyperparameter_tuning_rf(grid_search).csv\"\n",
    "\n",
    "# Check if the file exists and remove it once\n",
    "if os.path.exists(result_path):\n",
    "    print(f\"File {result_path} exists. Deleting it to start fresh.\")\n",
    "    os.remove(result_path)\n",
    "    \n",
    "\n",
    "run_classification(df_combined, y_target, ks, n_seeds,result_path,\"combined\", \"rf\", None,use_grid=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the results\n",
    "df = pd.read_csv(\"HP_analysis_10_2025/benchmark/results/Hyperparameter_tuning_rf(grid_search).csv\")\n",
    "\n",
    "# Keep only Macro scores and RF model\n",
    "df_macro = df[(df['Class'] == 'Macro') & (df['Model'] == 'rf')]\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['AUC', 'Precision', 'Recall', 'F1', 'MCC', 'PR']\n",
    "\n",
    "# Melt the dataframe for seaborn plotting\n",
    "df_long = df_macro.melt(id_vars=['Features (k)'], value_vars=metrics, var_name='Metric', value_name='Score')\n",
    "\n",
    "# Plot grouped barplot with feature count as x-axis\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=df_long, x='Features (k)', y='Score', hue='Metric', ci=None)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Random Forest Macro Scores Across Different Feature Counts\")\n",
    "plt.xlabel(\"Number of Features (k)\")\n",
    "plt.ylabel(\"Macro Scores\")\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File HP_analysis_10_2025/benchmark/results/testing.csv exists. Deleting it to start fresh.\n",
      "[10, 68267]\n",
      "2\n",
      "CV for seed 0 and 10 features\n",
      "Performing One vs Rest classification\n",
      "Macro AUC (OvR): 0.6698\n",
      "Macro Precision (OvR): 0.4195\n",
      "Macro Recall (OvR): 0.3926\n",
      "Macro F1 (OvR): 0.3289\n",
      "Macro MCC (OvR): 0.0719\n",
      "Macro PR AUC (OvR): 0.4313\n",
      "Performing One vs One classification\n",
      "Macro AUC (OvO): 0.6592\n",
      "Macro Precision (OvO): 0.4858\n",
      "Macro Recall (OvO): 0.2667\n",
      "Macro F1 (OvO): 0.3416\n",
      "Macro MCC (OvO): 0.2072\n",
      "Macro PR AUC (OvO): 0.4500\n",
      "{'Negative control vs Rest - Precision': 0.29411764705882354, 'Negative control vs Rest - Recall': 0.7777777777777778, 'Negative control vs Rest - F1': 0.4268292682926829, 'Negative control vs Rest - MCC': np.float64(-0.2487145971214342), 'Negative control vs Rest - PR AUC': np.float64(0.5004385697013709), 'Negative control vs Rest - AUC': np.float64(0.5749063670411986), 'Patient vs Rest - Precision': 0.7142857142857143, 'Patient vs Rest - Recall': 0.3333333333333333, 'Patient vs Rest - F1': 0.45454545454545453, 'Patient vs Rest - MCC': np.float64(0.44851653370375394), 'Patient vs Rest - PR AUC': np.float64(0.5225216127243658), 'Patient vs Rest - AUC': np.float64(0.8823529411764706), 'Positive control vs Rest - Precision': 0.25, 'Positive control vs Rest - Recall': 0.06666666666666667, 'Positive control vs Rest - F1': 0.10526315789473684, 'Positive control vs Rest - MCC': np.float64(0.015788848890522365), 'Positive control vs Rest - PR AUC': np.float64(0.2710716615761979), 'Positive control vs Rest - AUC': np.float64(0.5520833333333334), 'OvR Macro AUC': np.float64(0.6697808805170009), 'OvR Macro Precision': np.float64(0.41946778711484595), 'OvR Macro Recall': np.float64(0.3925925925925926), 'OvR Macro F1': np.float64(0.32887929357762474), 'OvR Macro MCC': np.float64(0.07186359515761404), 'OvR Macro PR AUC': np.float64(0.43134394800064485), 'Negative control vs Patient - Precision': 0.625, 'Negative control vs Patient - Recall': 0.3333333333333333, 'Negative control vs Patient - F1': 0.43478260869565216, 'Negative control vs Patient - MCC': np.float64(0.395037885949981), 'Negative control vs Patient - PR AUC': np.float64(0.5344294920097371), 'Negative control vs Patient - AUC': np.float64(0.747940074906367), 'Negative control vs Positive control - Precision': 0.2608695652173913, 'Negative control vs Positive control - Recall': 0.2, 'Negative control vs Positive control - F1': 0.22641509433962265, 'Negative control vs Positive control - MCC': np.float64(0.009884531198304698), 'Negative control vs Positive control - PR AUC': np.float64(0.2577652805227514), 'Negative control vs Positive control - AUC': np.float64(0.4940074906367041), 'Patient vs Positive control - Precision': 0.5714285714285714, 'Patient vs Positive control - Recall': 0.26666666666666666, 'Patient vs Positive control - F1': 0.36363636363636365, 'Patient vs Positive control - MCC': np.float64(0.21677749238102997), 'Patient vs Positive control - PR AUC': np.float64(0.6140568323967504), 'Patient vs Positive control - AUC': np.float64(0.7355555555555555), 'OvO Macro AUC': np.float64(0.6591677070328755), 'OvO Macro Precision': np.float64(0.48576604554865427), 'OvO Macro Recall': np.float64(0.26666666666666666), 'OvO Macro F1': np.float64(0.34161135555721284), 'OvO Macro MCC': np.float64(0.20723330317643854), 'OvO Macro PR AUC': np.float64(0.45004724148852887)}\n",
      "CV for seed 0 and 68267 features\n",
      "Performing One vs Rest classification\n",
      "Macro AUC (OvR): 0.6811\n",
      "Macro Precision (OvR): 0.4902\n",
      "Macro Recall (OvR): 0.3593\n",
      "Macro F1 (OvR): 0.2786\n",
      "Macro MCC (OvR): 0.0485\n",
      "Macro PR AUC (OvR): 0.5135\n",
      "Performing One vs One classification\n",
      "Macro AUC (OvO): 0.7295\n",
      "Macro Precision (OvO): 0.6667\n",
      "Macro Recall (OvO): 0.1778\n",
      "Macro F1 (OvO): 0.2778\n",
      "Macro MCC (OvO): 0.2755\n",
      "Macro PR AUC (OvO): 0.5536\n",
      "{'Negative control vs Rest - Precision': 0.304, 'Negative control vs Rest - Recall': 0.8444444444444444, 'Negative control vs Rest - F1': 0.4470588235294118, 'Negative control vs Rest - MCC': np.float64(-0.25110172001928444), 'Negative control vs Rest - PR AUC': np.float64(0.5906174593956379), 'Negative control vs Rest - AUC': np.float64(0.6530586766541823), 'Patient vs Rest - Precision': 1.0, 'Patient vs Rest - Recall': 0.2, 'Patient vs Rest - F1': 0.3333333333333333, 'Patient vs Rest - MCC': np.float64(0.4262386530020208), 'Patient vs Rest - PR AUC': np.float64(0.7163926001125546), 'Patient vs Rest - AUC': np.float64(0.8773109243697479), 'Positive control vs Rest - Precision': 0.16666666666666666, 'Positive control vs Rest - Recall': 0.03333333333333333, 'Positive control vs Rest - F1': 0.05555555555555555, 'Positive control vs Rest - MCC': np.float64(-0.02971665516199786), 'Positive control vs Rest - PR AUC': np.float64(0.23349152843221813), 'Positive control vs Rest - AUC': np.float64(0.5128205128205128), 'OvR Macro AUC': np.float64(0.681063371281481), 'OvR Macro Precision': np.float64(0.4902222222222223), 'OvR Macro Recall': np.float64(0.35925925925925933), 'OvR Macro F1': np.float64(0.2786492374727669), 'OvR Macro MCC': np.float64(0.048473425940246166), 'OvR Macro PR AUC': np.float64(0.5135005293134702), 'Negative control vs Patient - Precision': 1.0, 'Negative control vs Patient - Recall': 0.2, 'Negative control vs Patient - F1': 0.3333333333333333, 'Negative control vs Patient - MCC': np.float64(0.4198066504503904), 'Negative control vs Patient - PR AUC': np.float64(0.7382132106426925), 'Negative control vs Patient - AUC': np.float64(0.8970037453183521), 'Negative control vs Positive control - Precision': 0.0, 'Negative control vs Positive control - Recall': 0.0, 'Negative control vs Positive control - F1': 0.0, 'Negative control vs Positive control - MCC': np.float64(-0.09336782816736824), 'Negative control vs Positive control - PR AUC': np.float64(0.2678858232190189), 'Negative control vs Positive control - AUC': np.float64(0.5116104868913858), 'Patient vs Positive control - Precision': 1.0, 'Patient vs Positive control - Recall': 0.3333333333333333, 'Patient vs Positive control - F1': 0.5, 'Patient vs Positive control - MCC': np.float64(0.5), 'Patient vs Positive control - PR AUC': np.float64(0.7747369771150259), 'Patient vs Positive control - AUC': np.float64(0.78), 'OvO Macro AUC': np.float64(0.7295380774032459), 'OvO Macro Precision': np.float64(0.6666666666666666), 'OvO Macro Recall': np.float64(0.17777777777777778), 'OvO Macro F1': np.float64(0.27777777777777773), 'OvO Macro MCC': np.float64(0.275479607427674), 'OvO Macro PR AUC': np.float64(0.5535562664861913)}\n",
      "CV for seed 1 and 10 features\n",
      "Performing One vs Rest classification\n",
      "Macro AUC (OvR): 0.5655\n",
      "Macro Precision (OvR): 0.2777\n",
      "Macro Recall (OvR): 0.3407\n",
      "Macro F1 (OvR): 0.2702\n",
      "Macro MCC (OvR): -0.0195\n",
      "Macro PR AUC (OvR): 0.3475\n",
      "Performing One vs One classification\n",
      "Macro AUC (OvO): 0.6870\n",
      "Macro Precision (OvO): 0.5835\n",
      "Macro Recall (OvO): 0.3111\n",
      "Macro F1 (OvO): 0.3977\n",
      "Macro MCC (OvO): 0.3016\n",
      "Macro PR AUC (OvO): 0.4195\n",
      "{'Negative control vs Rest - Precision': 0.27927927927927926, 'Negative control vs Rest - Recall': 0.6888888888888889, 'Negative control vs Rest - F1': 0.3974358974358974, 'Negative control vs Rest - MCC': np.float64(-0.26300822371917737), 'Negative control vs Rest - PR AUC': np.float64(0.5709220627855826), 'Negative control vs Rest - AUC': np.float64(0.5941323345817728), 'Patient vs Rest - Precision': 0.4, 'Patient vs Rest - Recall': 0.26666666666666666, 'Patient vs Rest - F1': 0.32, 'Patient vs Rest - MCC': np.float64(0.25945219657184376), 'Patient vs Rest - PR AUC': np.float64(0.2882450028641088), 'Patient vs Rest - AUC': np.float64(0.7011204481792718), 'Positive control vs Rest - Precision': 0.15384615384615385, 'Positive control vs Rest - Recall': 0.06666666666666667, 'Positive control vs Rest - F1': 0.09302325581395349, 'Positive control vs Rest - MCC': np.float64(-0.05507039256845044), 'Positive control vs Rest - PR AUC': np.float64(0.18320285812903658), 'Positive control vs Rest - AUC': np.float64(0.4012820512820513), 'OvR Macro AUC': np.float64(0.5655116113476986), 'OvR Macro Precision': np.float64(0.2777084777084777), 'OvR Macro Recall': np.float64(0.34074074074074073), 'OvR Macro F1': np.float64(0.27015305108328364), 'OvR Macro MCC': np.float64(-0.01954213990526135), 'OvR Macro PR AUC': np.float64(0.34745664125957604), 'Negative control vs Patient - Precision': 0.7142857142857143, 'Negative control vs Patient - Recall': 0.3333333333333333, 'Negative control vs Patient - F1': 0.45454545454545453, 'Negative control vs Patient - MCC': np.float64(0.43588565510354077), 'Negative control vs Patient - PR AUC': np.float64(0.425999743293682), 'Negative control vs Patient - AUC': np.float64(0.7711610486891386), 'Negative control vs Positive control - Precision': 0.4, 'Negative control vs Positive control - Recall': 0.13333333333333333, 'Negative control vs Positive control - F1': 0.2, 'Negative control vs Positive control - MCC': np.float64(0.10316770940374433), 'Negative control vs Positive control - PR AUC': np.float64(0.37252790255882184), 'Negative control vs Positive control - AUC': np.float64(0.5397003745318352), 'Patient vs Positive control - Precision': 0.6363636363636364, 'Patient vs Positive control - Recall': 0.4666666666666667, 'Patient vs Positive control - F1': 0.5384615384615384, 'Patient vs Positive control - MCC': np.float64(0.3656362120635653), 'Patient vs Positive control - PR AUC': np.float64(0.6762127723992665), 'Patient vs Positive control - AUC': np.float64(0.75), 'OvO Macro AUC': np.float64(0.6869538077403247), 'OvO Macro Precision': np.float64(0.5835497835497835), 'OvO Macro Recall': np.float64(0.3111111111111111), 'OvO Macro F1': np.float64(0.3976689976689977), 'OvO Macro MCC': np.float64(0.3015631921902835), 'OvO Macro PR AUC': np.float64(0.41951839033841637)}\n",
      "CV for seed 1 and 68267 features\n",
      "Performing One vs Rest classification\n",
      "Macro AUC (OvR): 0.6820\n",
      "Macro Precision (OvR): 0.4385\n",
      "Macro Recall (OvR): 0.3370\n",
      "Macro F1 (OvR): 0.2162\n",
      "Macro MCC (OvR): -0.0046\n",
      "Macro PR AUC (OvR): 0.5076\n",
      "Performing One vs One classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renu/miniconda3/envs/planemo-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro AUC (OvO): 0.7391\n",
      "Macro Precision (OvO): 0.4444\n",
      "Macro Recall (OvO): 0.1333\n",
      "Macro F1 (OvO): 0.1979\n",
      "Macro MCC (OvO): 0.1859\n",
      "Macro PR AUC (OvO): 0.5265\n",
      "{'Negative control vs Rest - Precision': 0.3153846153846154, 'Negative control vs Rest - Recall': 0.9111111111111111, 'Negative control vs Rest - F1': 0.4685714285714286, 'Negative control vs Rest - MCC': np.float64(-0.24668745581139884), 'Negative control vs Rest - PR AUC': np.float64(0.6658272755275458), 'Negative control vs Rest - AUC': np.float64(0.7204744069912609), 'Patient vs Rest - Precision': 0.5, 'Patient vs Rest - Recall': 0.06666666666666667, 'Patient vs Rest - F1': 0.11764705882352941, 'Patient vs Rest - MCC': np.float64(0.15149987190590394), 'Patient vs Rest - PR AUC': np.float64(0.6076966297607611), 'Patient vs Rest - AUC': np.float64(0.7983193277310925), 'Positive control vs Rest - Precision': 0.5, 'Positive control vs Rest - Recall': 0.03333333333333333, 'Positive control vs Rest - F1': 0.0625, 'Positive control vs Rest - MCC': np.float64(0.08153657399114153), 'Positive control vs Rest - PR AUC': np.float64(0.24929656573392778), 'Positive control vs Rest - AUC': np.float64(0.5270833333333333), 'OvR Macro AUC': np.float64(0.6819590226852288), 'OvR Macro Precision': np.float64(0.43846153846153846), 'OvR Macro Recall': np.float64(0.337037037037037), 'OvR Macro F1': np.float64(0.21623949579831933), 'OvR Macro MCC': np.float64(-0.004550336638117787), 'OvR Macro PR AUC': np.float64(0.5076068236740783), 'Negative control vs Patient - Precision': 0.5, 'Negative control vs Patient - Recall': 0.06666666666666667, 'Negative control vs Patient - F1': 0.11764705882352941, 'Negative control vs Patient - MCC': np.float64(0.14179992566122718), 'Negative control vs Patient - PR AUC': np.float64(0.55361914271199), 'Negative control vs Patient - AUC': np.float64(0.8265917602996256), 'Negative control vs Positive control - Precision': 0.0, 'Negative control vs Positive control - Recall': 0.0, 'Negative control vs Positive control - F1': 0.0, 'Negative control vs Positive control - MCC': 0.0, 'Negative control vs Positive control - PR AUC': np.float64(0.2982096482381317), 'Negative control vs Positive control - AUC': np.float64(0.5719101123595507), 'Patient vs Positive control - Precision': 0.8333333333333334, 'Patient vs Positive control - Recall': 0.3333333333333333, 'Patient vs Positive control - F1': 0.47619047619047616, 'Patient vs Positive control - MCC': np.float64(0.41602514716892186), 'Patient vs Positive control - PR AUC': np.float64(0.7845592319951546), 'Patient vs Positive control - AUC': np.float64(0.8188888888888889), 'OvO Macro AUC': np.float64(0.739130253849355), 'OvO Macro Precision': np.float64(0.4444444444444445), 'OvO Macro Recall': np.float64(0.13333333333333333), 'OvO Macro F1': np.float64(0.19794584500466852), 'OvO Macro MCC': np.float64(0.185941690943383), 'OvO Macro PR AUC': np.float64(0.5265347489945852)}\n",
      "just checking the values of result :  dict_items([('Negative control vs Rest - Precision', 0.29411764705882354), ('Negative control vs Rest - Recall', 0.7777777777777778), ('Negative control vs Rest - F1', 0.4268292682926829), ('Negative control vs Rest - MCC', np.float64(-0.2487145971214342)), ('Negative control vs Rest - PR AUC', np.float64(0.5004385697013709)), ('Negative control vs Rest - AUC', np.float64(0.5749063670411986)), ('Patient vs Rest - Precision', 0.7142857142857143), ('Patient vs Rest - Recall', 0.3333333333333333), ('Patient vs Rest - F1', 0.45454545454545453), ('Patient vs Rest - MCC', np.float64(0.44851653370375394)), ('Patient vs Rest - PR AUC', np.float64(0.5225216127243658)), ('Patient vs Rest - AUC', np.float64(0.8823529411764706)), ('Positive control vs Rest - Precision', 0.25), ('Positive control vs Rest - Recall', 0.06666666666666667), ('Positive control vs Rest - F1', 0.10526315789473684), ('Positive control vs Rest - MCC', np.float64(0.015788848890522365)), ('Positive control vs Rest - PR AUC', np.float64(0.2710716615761979)), ('Positive control vs Rest - AUC', np.float64(0.5520833333333334)), ('OvR Macro AUC', np.float64(0.6697808805170009)), ('OvR Macro Precision', np.float64(0.41946778711484595)), ('OvR Macro Recall', np.float64(0.3925925925925926)), ('OvR Macro F1', np.float64(0.32887929357762474)), ('OvR Macro MCC', np.float64(0.07186359515761404)), ('OvR Macro PR AUC', np.float64(0.43134394800064485)), ('Negative control vs Patient - Precision', 0.625), ('Negative control vs Patient - Recall', 0.3333333333333333), ('Negative control vs Patient - F1', 0.43478260869565216), ('Negative control vs Patient - MCC', np.float64(0.395037885949981)), ('Negative control vs Patient - PR AUC', np.float64(0.5344294920097371)), ('Negative control vs Patient - AUC', np.float64(0.747940074906367)), ('Negative control vs Positive control - Precision', 0.2608695652173913), ('Negative control vs Positive control - Recall', 0.2), ('Negative control vs Positive control - F1', 0.22641509433962265), ('Negative control vs Positive control - MCC', np.float64(0.009884531198304698)), ('Negative control vs Positive control - PR AUC', np.float64(0.2577652805227514)), ('Negative control vs Positive control - AUC', np.float64(0.4940074906367041)), ('Patient vs Positive control - Precision', 0.5714285714285714), ('Patient vs Positive control - Recall', 0.26666666666666666), ('Patient vs Positive control - F1', 0.36363636363636365), ('Patient vs Positive control - MCC', np.float64(0.21677749238102997)), ('Patient vs Positive control - PR AUC', np.float64(0.6140568323967504)), ('Patient vs Positive control - AUC', np.float64(0.7355555555555555)), ('OvO Macro AUC', np.float64(0.6591677070328755)), ('OvO Macro Precision', np.float64(0.48576604554865427)), ('OvO Macro Recall', np.float64(0.26666666666666666)), ('OvO Macro F1', np.float64(0.34161135555721284)), ('OvO Macro MCC', np.float64(0.20723330317643854)), ('OvO Macro PR AUC', np.float64(0.45004724148852887))])\n",
      "just checking the values of result :  dict_items([('Negative control vs Rest - Precision', 0.304), ('Negative control vs Rest - Recall', 0.8444444444444444), ('Negative control vs Rest - F1', 0.4470588235294118), ('Negative control vs Rest - MCC', np.float64(-0.25110172001928444)), ('Negative control vs Rest - PR AUC', np.float64(0.5906174593956379)), ('Negative control vs Rest - AUC', np.float64(0.6530586766541823)), ('Patient vs Rest - Precision', 1.0), ('Patient vs Rest - Recall', 0.2), ('Patient vs Rest - F1', 0.3333333333333333), ('Patient vs Rest - MCC', np.float64(0.4262386530020208)), ('Patient vs Rest - PR AUC', np.float64(0.7163926001125546)), ('Patient vs Rest - AUC', np.float64(0.8773109243697479)), ('Positive control vs Rest - Precision', 0.16666666666666666), ('Positive control vs Rest - Recall', 0.03333333333333333), ('Positive control vs Rest - F1', 0.05555555555555555), ('Positive control vs Rest - MCC', np.float64(-0.02971665516199786)), ('Positive control vs Rest - PR AUC', np.float64(0.23349152843221813)), ('Positive control vs Rest - AUC', np.float64(0.5128205128205128)), ('OvR Macro AUC', np.float64(0.681063371281481)), ('OvR Macro Precision', np.float64(0.4902222222222223)), ('OvR Macro Recall', np.float64(0.35925925925925933)), ('OvR Macro F1', np.float64(0.2786492374727669)), ('OvR Macro MCC', np.float64(0.048473425940246166)), ('OvR Macro PR AUC', np.float64(0.5135005293134702)), ('Negative control vs Patient - Precision', 1.0), ('Negative control vs Patient - Recall', 0.2), ('Negative control vs Patient - F1', 0.3333333333333333), ('Negative control vs Patient - MCC', np.float64(0.4198066504503904)), ('Negative control vs Patient - PR AUC', np.float64(0.7382132106426925)), ('Negative control vs Patient - AUC', np.float64(0.8970037453183521)), ('Negative control vs Positive control - Precision', 0.0), ('Negative control vs Positive control - Recall', 0.0), ('Negative control vs Positive control - F1', 0.0), ('Negative control vs Positive control - MCC', np.float64(-0.09336782816736824)), ('Negative control vs Positive control - PR AUC', np.float64(0.2678858232190189)), ('Negative control vs Positive control - AUC', np.float64(0.5116104868913858)), ('Patient vs Positive control - Precision', 1.0), ('Patient vs Positive control - Recall', 0.3333333333333333), ('Patient vs Positive control - F1', 0.5), ('Patient vs Positive control - MCC', np.float64(0.5)), ('Patient vs Positive control - PR AUC', np.float64(0.7747369771150259)), ('Patient vs Positive control - AUC', np.float64(0.78)), ('OvO Macro AUC', np.float64(0.7295380774032459)), ('OvO Macro Precision', np.float64(0.6666666666666666)), ('OvO Macro Recall', np.float64(0.17777777777777778)), ('OvO Macro F1', np.float64(0.27777777777777773)), ('OvO Macro MCC', np.float64(0.275479607427674)), ('OvO Macro PR AUC', np.float64(0.5535562664861913))])\n",
      "just checking the values of result :  dict_items([('Negative control vs Rest - Precision', 0.27927927927927926), ('Negative control vs Rest - Recall', 0.6888888888888889), ('Negative control vs Rest - F1', 0.3974358974358974), ('Negative control vs Rest - MCC', np.float64(-0.26300822371917737)), ('Negative control vs Rest - PR AUC', np.float64(0.5709220627855826)), ('Negative control vs Rest - AUC', np.float64(0.5941323345817728)), ('Patient vs Rest - Precision', 0.4), ('Patient vs Rest - Recall', 0.26666666666666666), ('Patient vs Rest - F1', 0.32), ('Patient vs Rest - MCC', np.float64(0.25945219657184376)), ('Patient vs Rest - PR AUC', np.float64(0.2882450028641088)), ('Patient vs Rest - AUC', np.float64(0.7011204481792718)), ('Positive control vs Rest - Precision', 0.15384615384615385), ('Positive control vs Rest - Recall', 0.06666666666666667), ('Positive control vs Rest - F1', 0.09302325581395349), ('Positive control vs Rest - MCC', np.float64(-0.05507039256845044)), ('Positive control vs Rest - PR AUC', np.float64(0.18320285812903658)), ('Positive control vs Rest - AUC', np.float64(0.4012820512820513)), ('OvR Macro AUC', np.float64(0.5655116113476986)), ('OvR Macro Precision', np.float64(0.2777084777084777)), ('OvR Macro Recall', np.float64(0.34074074074074073)), ('OvR Macro F1', np.float64(0.27015305108328364)), ('OvR Macro MCC', np.float64(-0.01954213990526135)), ('OvR Macro PR AUC', np.float64(0.34745664125957604)), ('Negative control vs Patient - Precision', 0.7142857142857143), ('Negative control vs Patient - Recall', 0.3333333333333333), ('Negative control vs Patient - F1', 0.45454545454545453), ('Negative control vs Patient - MCC', np.float64(0.43588565510354077)), ('Negative control vs Patient - PR AUC', np.float64(0.425999743293682)), ('Negative control vs Patient - AUC', np.float64(0.7711610486891386)), ('Negative control vs Positive control - Precision', 0.4), ('Negative control vs Positive control - Recall', 0.13333333333333333), ('Negative control vs Positive control - F1', 0.2), ('Negative control vs Positive control - MCC', np.float64(0.10316770940374433)), ('Negative control vs Positive control - PR AUC', np.float64(0.37252790255882184)), ('Negative control vs Positive control - AUC', np.float64(0.5397003745318352)), ('Patient vs Positive control - Precision', 0.6363636363636364), ('Patient vs Positive control - Recall', 0.4666666666666667), ('Patient vs Positive control - F1', 0.5384615384615384), ('Patient vs Positive control - MCC', np.float64(0.3656362120635653)), ('Patient vs Positive control - PR AUC', np.float64(0.6762127723992665)), ('Patient vs Positive control - AUC', np.float64(0.75)), ('OvO Macro AUC', np.float64(0.6869538077403247)), ('OvO Macro Precision', np.float64(0.5835497835497835)), ('OvO Macro Recall', np.float64(0.3111111111111111)), ('OvO Macro F1', np.float64(0.3976689976689977)), ('OvO Macro MCC', np.float64(0.3015631921902835)), ('OvO Macro PR AUC', np.float64(0.41951839033841637))])\n",
      "just checking the values of result :  dict_items([('Negative control vs Rest - Precision', 0.3153846153846154), ('Negative control vs Rest - Recall', 0.9111111111111111), ('Negative control vs Rest - F1', 0.4685714285714286), ('Negative control vs Rest - MCC', np.float64(-0.24668745581139884)), ('Negative control vs Rest - PR AUC', np.float64(0.6658272755275458)), ('Negative control vs Rest - AUC', np.float64(0.7204744069912609)), ('Patient vs Rest - Precision', 0.5), ('Patient vs Rest - Recall', 0.06666666666666667), ('Patient vs Rest - F1', 0.11764705882352941), ('Patient vs Rest - MCC', np.float64(0.15149987190590394)), ('Patient vs Rest - PR AUC', np.float64(0.6076966297607611)), ('Patient vs Rest - AUC', np.float64(0.7983193277310925)), ('Positive control vs Rest - Precision', 0.5), ('Positive control vs Rest - Recall', 0.03333333333333333), ('Positive control vs Rest - F1', 0.0625), ('Positive control vs Rest - MCC', np.float64(0.08153657399114153)), ('Positive control vs Rest - PR AUC', np.float64(0.24929656573392778)), ('Positive control vs Rest - AUC', np.float64(0.5270833333333333)), ('OvR Macro AUC', np.float64(0.6819590226852288)), ('OvR Macro Precision', np.float64(0.43846153846153846)), ('OvR Macro Recall', np.float64(0.337037037037037)), ('OvR Macro F1', np.float64(0.21623949579831933)), ('OvR Macro MCC', np.float64(-0.004550336638117787)), ('OvR Macro PR AUC', np.float64(0.5076068236740783)), ('Negative control vs Patient - Precision', 0.5), ('Negative control vs Patient - Recall', 0.06666666666666667), ('Negative control vs Patient - F1', 0.11764705882352941), ('Negative control vs Patient - MCC', np.float64(0.14179992566122718)), ('Negative control vs Patient - PR AUC', np.float64(0.55361914271199)), ('Negative control vs Patient - AUC', np.float64(0.8265917602996256)), ('Negative control vs Positive control - Precision', 0.0), ('Negative control vs Positive control - Recall', 0.0), ('Negative control vs Positive control - F1', 0.0), ('Negative control vs Positive control - MCC', 0.0), ('Negative control vs Positive control - PR AUC', np.float64(0.2982096482381317)), ('Negative control vs Positive control - AUC', np.float64(0.5719101123595507)), ('Patient vs Positive control - Precision', 0.8333333333333334), ('Patient vs Positive control - Recall', 0.3333333333333333), ('Patient vs Positive control - F1', 0.47619047619047616), ('Patient vs Positive control - MCC', np.float64(0.41602514716892186)), ('Patient vs Positive control - PR AUC', np.float64(0.7845592319951546)), ('Patient vs Positive control - AUC', np.float64(0.8188888888888889)), ('OvO Macro AUC', np.float64(0.739130253849355)), ('OvO Macro Precision', np.float64(0.4444444444444445)), ('OvO Macro Recall', np.float64(0.13333333333333333)), ('OvO Macro F1', np.float64(0.19794584500466852)), ('OvO Macro MCC', np.float64(0.185941690943383)), ('OvO Macro PR AUC', np.float64(0.5265347489945852))])\n",
      "    Seed  Features (k)     Label Model Sampling_Strategy  \\\n",
      "0      0            10  combined    rf              None   \n",
      "1      0            10  combined    rf              None   \n",
      "2      0            10  combined    rf              None   \n",
      "3      0            10  combined    rf              None   \n",
      "4      0            10  combined    rf              None   \n",
      "5      0            10  combined    rf              None   \n",
      "6      0            10  combined    rf              None   \n",
      "7      0            10  combined    rf              None   \n",
      "8      0         68267  combined    rf              None   \n",
      "9      0         68267  combined    rf              None   \n",
      "10     0         68267  combined    rf              None   \n",
      "11     0         68267  combined    rf              None   \n",
      "12     0         68267  combined    rf              None   \n",
      "13     0         68267  combined    rf              None   \n",
      "14     0         68267  combined    rf              None   \n",
      "15     0         68267  combined    rf              None   \n",
      "16     1            10  combined    rf              None   \n",
      "17     1            10  combined    rf              None   \n",
      "18     1            10  combined    rf              None   \n",
      "19     1            10  combined    rf              None   \n",
      "20     1            10  combined    rf              None   \n",
      "21     1            10  combined    rf              None   \n",
      "22     1            10  combined    rf              None   \n",
      "23     1            10  combined    rf              None   \n",
      "24     1         68267  combined    rf              None   \n",
      "25     1         68267  combined    rf              None   \n",
      "26     1         68267  combined    rf              None   \n",
      "27     1         68267  combined    rf              None   \n",
      "28     1         68267  combined    rf              None   \n",
      "29     1         68267  combined    rf              None   \n",
      "30     1         68267  combined    rf              None   \n",
      "31     1         68267  combined    rf              None   \n",
      "\n",
      "                                   Class Type       AUC  Precision    Recall  \\\n",
      "0                                Patient  OvR  0.522522   0.714286  0.333333   \n",
      "1            Negative control vs Patient  OvO  0.534429   0.625000  0.333333   \n",
      "2   Negative control vs Positive control  OvO  0.257765   0.260870  0.200000   \n",
      "3                       Negative control  OvR  0.500439   0.294118  0.777778   \n",
      "4            Patient vs Positive control  OvO  0.614057   0.571429  0.266667   \n",
      "5                                  Macro  OvO  0.659168   0.485766  0.266667   \n",
      "6                                  Macro  OvR  0.669781   0.419468  0.392593   \n",
      "7                       Positive control  OvR  0.271072   0.250000  0.066667   \n",
      "8                                Patient  OvR  0.716393   1.000000  0.200000   \n",
      "9            Negative control vs Patient  OvO  0.738213   1.000000  0.200000   \n",
      "10  Negative control vs Positive control  OvO  0.267886   0.000000  0.000000   \n",
      "11                      Negative control  OvR  0.590617   0.304000  0.844444   \n",
      "12           Patient vs Positive control  OvO  0.774737   1.000000  0.333333   \n",
      "13                                 Macro  OvO  0.729538   0.666667  0.177778   \n",
      "14                                 Macro  OvR  0.681063   0.490222  0.359259   \n",
      "15                      Positive control  OvR  0.233492   0.166667  0.033333   \n",
      "16                               Patient  OvR  0.288245   0.400000  0.266667   \n",
      "17           Negative control vs Patient  OvO  0.426000   0.714286  0.333333   \n",
      "18  Negative control vs Positive control  OvO  0.372528   0.400000  0.133333   \n",
      "19                      Negative control  OvR  0.570922   0.279279  0.688889   \n",
      "20           Patient vs Positive control  OvO  0.676213   0.636364  0.466667   \n",
      "21                                 Macro  OvO  0.686954   0.583550  0.311111   \n",
      "22                                 Macro  OvR  0.565512   0.277708  0.340741   \n",
      "23                      Positive control  OvR  0.183203   0.153846  0.066667   \n",
      "24                               Patient  OvR  0.607697   0.500000  0.066667   \n",
      "25           Negative control vs Patient  OvO  0.553619   0.500000  0.066667   \n",
      "26  Negative control vs Positive control  OvO  0.298210   0.000000  0.000000   \n",
      "27                      Negative control  OvR  0.665827   0.315385  0.911111   \n",
      "28           Patient vs Positive control  OvO  0.784559   0.833333  0.333333   \n",
      "29                                 Macro  OvO  0.739130   0.444444  0.133333   \n",
      "30                                 Macro  OvR  0.681959   0.438462  0.337037   \n",
      "31                      Positive control  OvR  0.249297   0.500000  0.033333   \n",
      "\n",
      "          F1       MCC        PR  \n",
      "0   0.454545  0.448517  0.522522  \n",
      "1   0.434783  0.395038  0.534429  \n",
      "2   0.226415  0.009885  0.257765  \n",
      "3   0.426829 -0.248715  0.500439  \n",
      "4   0.363636  0.216777  0.614057  \n",
      "5   0.341611  0.207233  0.450047  \n",
      "6   0.328879  0.071864  0.431344  \n",
      "7   0.105263  0.015789  0.271072  \n",
      "8   0.333333  0.426239  0.716393  \n",
      "9   0.333333  0.419807  0.738213  \n",
      "10  0.000000 -0.093368  0.267886  \n",
      "11  0.447059 -0.251102  0.590617  \n",
      "12  0.500000  0.500000  0.774737  \n",
      "13  0.277778  0.275480  0.553556  \n",
      "14  0.278649  0.048473  0.513501  \n",
      "15  0.055556 -0.029717  0.233492  \n",
      "16  0.320000  0.259452  0.288245  \n",
      "17  0.454545  0.435886  0.426000  \n",
      "18  0.200000  0.103168  0.372528  \n",
      "19  0.397436 -0.263008  0.570922  \n",
      "20  0.538462  0.365636  0.676213  \n",
      "21  0.397669  0.301563  0.419518  \n",
      "22  0.270153 -0.019542  0.347457  \n",
      "23  0.093023 -0.055070  0.183203  \n",
      "24  0.117647  0.151500  0.607697  \n",
      "25  0.117647  0.141800  0.553619  \n",
      "26  0.000000  0.000000  0.298210  \n",
      "27  0.468571 -0.246687  0.665827  \n",
      "28  0.476190  0.416025  0.784559  \n",
      "29  0.197946  0.185942  0.526535  \n",
      "30  0.216239 -0.004550  0.507607  \n",
      "31  0.062500  0.081537  0.249297  \n"
     ]
    }
   ],
   "source": [
    "n_seeds = 2\n",
    "ks = [10]\n",
    "result_path=\"HP_analysis_10_2025/benchmark/results/testing.csv\"\n",
    "# Check if the file exists and remove it once\n",
    "if os.path.exists(result_path):\n",
    "    print(f\"File {result_path} exists. Deleting it to start fresh.\")\n",
    "    os.remove(result_path)\n",
    "    \n",
    "\n",
    "run_classification(df_combined, y_target, ks, n_seeds,result_path,\"combined\", \"rf\", None,use_grid=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
